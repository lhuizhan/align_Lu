#!/usr/bin/env python
import optparse
import sys
from math import log
from collections import defaultdict
import numpy as np

MAX_Iter = 20
MAX_Iter_Model1 = 100

optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()
f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

sys.stderr.write("Training with HMM...")
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))[:opts.num_sents]]
f_count = defaultdict(int)
e_count = defaultdict(int)
fe_count = defaultdict(int)
MAX_Sentence_Length = 0
alpha = 1.0

for (n, (f, e)) in enumerate(bitext):
  if MAX_Sentence_Length < max(len(f), len(e)) + 10:
    MAX_Sentence_Length = max(len(f), len(e)) + 10
  for f_i in set(f):
    f_count[f_i] += 1
    for e_j in set(e):
      fe_count[(f_i, e_j)] += 1
  for e_j in set(e):
    e_count[e_j] += 1
  if n % 500 == 0:
    sys.stderr.write(".")

#Init
#IBM1_t = t(f_j | e_i)
IBM1_t = defaultdict(float)

word_count_e = len(e_count)
#Uniform Distribution
for (f_j, e_i) in fe_count.keys():
  IBM1_t[(f_j, e_i)] = 1.0/word_count_e
 
#IBM Model_1 Init

#IBM Model_1 EM
IBM1_converged = False
IBM1_count_iter = 0
while (not IBM1_converged):
  IBM1_converged = True
  IBM1_count_fe = defaultdict(float)
  IBM1_total_e = defaultdict(float)
  IBM1_total_f = defaultdict(float)
  for (f, e) in bitext:
    #Compute normalization
    for f_j in f:
      IBM1_total_f[f_j] = 0.0
      for e_i in e:
        IBM1_total_f[f_j] += IBM1_t[(f_j, e_i)]
    #Collect counts
    for f_j in f:
      for e_i in e:
        IBM1_count_fe[(f_j, e_i)] += IBM1_t[(f_j, e_i)]/IBM1_total_f[f_j]
        IBM1_total_e[e_i] += IBM1_t[(f_j, e_i)]/IBM1_total_f[f_j]

  for (f_j, e_i) in fe_count.keys():
    newt = IBM1_count_fe[(f_j, e_i)]/IBM1_total_e[e_i]
    if (abs(IBM1_t[(f_j, e_i)] - newt) > 1e-2):
      IBM1_converged = False
    IBM1_t[(f_j, e_i)] = newt
    # print (IBM1_t[(f_j, e_i)], newt)
  IBM1_count_iter += 1
  if (IBM1_count_iter > MAX_Iter_Model1):
    break
# print IBM1_t[("le", "the")]
#predict results, and collect counts for HMM
#HMM_ac[d] is the count of alignments with d = i - j
HMM_ac = defaultdict(int)
for (f, e) in bitext:
  last_index = -1
  for (j, f_j) in enumerate(f):
    MAX_t = 0
    MAX_index = 0
    for (i, e_i) in enumerate(e):
      if IBM1_t[(f_j, e_i)] > MAX_t:
        MAX_t = IBM1_t[(f_j, e_i)]
        MAX_index = i
    # sys.stdout.write("%i-%i " % (j, MAX_index))
    if last_index != -1:
      HMM_ac[last_index - MAX_index] += 1
    last_index = MAX_index
  # sys.stdout.write("\n")
#Strong prior
# HMM_ac[1] += 10000
# for i in HMM_ac.keys():
#   print i, HMM_ac[i]




#HMM Model Init 

#HMM_t is still t(f_j | e_i), the translation probability
#HMM_logp is the alignment probability, pa(a_j | a_(j-1), I),
#To save the whole HMM_logp would be a waste of memory, only save c(i, i')
#p(a_j | a_(j-1), I) = c(i - i')/Sum c(i'' - i'), i'' in range I
HMM_t = IBM1_t
HMM_p0 = 0.1
HMM_converged = False
HMM_Iter_count = 0
HMM_perp = 1e30
HMM_newperp = 0
#Init HMM_logp, the matrix that saves the maximal probabilities
#HMM_last_index saves the indices
#HMM_sum_ac is just Sum c(i'' - i')
HMM_logp = [[0 for i in range(MAX_Sentence_Length)] for i in range(MAX_Sentence_Length)]
HMM_last_index = [[0 for i in range(MAX_Sentence_Length)] for i in range(MAX_Sentence_Length)]
HMM_sum_ac = [[0 for i in range(MAX_Sentence_Length)] for i in range(MAX_Sentence_Length)]
for I in range(MAX_Sentence_Length):
  for iprime in range(I):
    for itemp in range(I):
      HMM_sum_ac[iprime][I] += HMM_ac[itemp - iprime]

# for iprime in range(10):
#   print HMM_ac[iprime - 5]
# print "!!!"
# print log(HMM_t[("le", "the")])

for i in range(MAX_Sentence_Length):
  HMM_logp[0][i] = 0
while (not HMM_converged):
  HMM_converged = True
  HMM_ac_new = defaultdict(int)
  HMM_count_fe = defaultdict(int)
  #Maximization using DP
  for (k, (f, e)) in enumerate(bitext):
    for (j, f_j) in enumerate(f):
      for (i, e_i) in enumerate(e):
        HMM_logp[j + 1][i] = 1e30
        # HMM_last_index[j + 1][i] = 32767
        for iprime in range(len(e)):
          if HMM_ac[i - iprime] == 0 :
            temp = 1e-8
          else:
            temp = float(HMM_ac[i - iprime])/float(HMM_sum_ac[iprime][len(e)])
          if HMM_t[(f_j, e_i)] == 0 :
            HMM_t[(f_j, e_i)] = 1e-10
          # print log(temp), log(HMM_t[(f_j, e_i)])
          # print HMM_ac[i - iprime], HMM_sum_ac[iprime][len(e)], -log(float(HMM_ac[i - iprime])/HMM_sum_ac[iprime][len(e)]), -log(HMM_t[(f_j, e_i)])
          if HMM_logp[j + 1][i] > HMM_logp[j][iprime] - alpha * log(temp) - log(HMM_t[(f_j, e_i)]):
            HMM_logp[j + 1][i] = HMM_logp[j][iprime] - alpha * log(temp) - log(HMM_t[(f_j, e_i)])
            HMM_last_index[j + 1][i] = iprime
          # if HMM_logp[j][iprime] * HMM_ac[i - iprime]/HMM_sum_ac[iprime][len(e)] * HMM_t[(f_j, e_i)] > HMM_logp[j + 1][i]:
          #   HMM_logp[j + 1][i] = HMM_logp[j][iprime] * HMM_ac[i - iprime]/HMM_sum_ac[iprime][len(e)] * HMM_t[(f_j, e_i)]
          #   HMM_last_index[j + 1][i] = iprime
          # if (HMM_last_index[j + 1][i] == iprime):
          #   print HMM_logp[j + 1][i]
    #Viterbi Version
    #Find the path with Maximum Probability
    curr_index = 0
    MIN_logp = 1e30
    for i in range(len(e)):
      if MIN_logp > HMM_logp[len(f)][i]:
        MIN_logp = HMM_logp[len(f)][i]
        curr_index = i
    for j in range(1, len(f) + 1)[::-1]:
      prev_index = HMM_last_index[j][curr_index]
      # print j, prev_index
      # print len(f) - j,prev_index
      # sys.stdout.write("%i-%i " % (j - 1,prev_index))
      # if k == 2:
      #   print j, curr_index, HMM_logp[j][curr_index]
      #   print len(f), j-1
      #   print len(e), curr_index
      #   print f[j - 1], e[curr_index]
      #   print HMM_logp[j][curr_index]
      HMM_count_fe[(f[j - 1], e[curr_index])] += 1
      if j != 1:
        HMM_ac_new[curr_index - prev_index] += 1
      HMM_newperp += HMM_logp[j][curr_index]
      curr_index = prev_index
    # sys.stdout.write("\n")
  #Estimation
  # if ("le", "the") in HMM_count_fe.keys():
  #   print "its here"
  # for (f_j, e_i) in HMM_count_fe.keys():
  #   HMM_t[(f_j, e_i)] = HMM_count_fe[(f_j, e_i)]/e_count[e_i]
  for i in HMM_ac.keys():
    HMM_ac[i] = HMM_ac_new[i]
  # HMM_ac[0] /= 3
  # print HMM_ac
  # Sum = 0
  # for (f_j) in f_count.keys():
  #   if (f_j, "the") in fe_count.keys(): 
  #     print f_j, HMM_t[f_j, "the"]
  #     Sum += HMM_t[(f_j, "the")]
  # print Sum
  # for iprime in range(10):
  #   print HMM_ac[iprime - 5]


  for I in range(MAX_Sentence_Length):
    for iprime in range(I):
      HMM_sum_ac[iprime][I] = 0
      for itemp in range(I):
        HMM_sum_ac[iprime][I] += HMM_ac[itemp - iprime]
  if abs(HMM_newperp - HMM_perp) > 1e-2:
    HMM_converged = False
  HMM_perp = HMM_newperp
  HMM_Iter_count += 1
  if HMM_Iter_count > MAX_Iter:
    break
  # print "!!!"
  # if (HMM_t[("le", "the")] > 0):
  #   print log(HMM_t[("le", "the")])

  # for i in range(10):
  #   print i, HMM_ac[5 - i], HMM_sum_ac[i][10], float(HMM_ac[5 - i])/HMM_sum_ac[i][10]

  #Using new data to construct new t and ac

#HMM Model prediction
for (k, (f, e)) in enumerate(bitext):
  for (j, f_j) in enumerate(f):
    for (i, e_i) in enumerate(e):
      HMM_logp[j + 1][i] = 1e30
      # HMM_last_index[j + 1][i] = 32767
      for iprime in range(len(e)):
        if HMM_ac[i - iprime] == 0 :
          temp = 1e-8
        else:
          temp = float(HMM_ac[i - iprime])/float(HMM_sum_ac[iprime][len(e)])
        if HMM_t[(f_j, e_i)] == 0:
          HMM_t[(f_j, e_i)] = 1e-10
        if HMM_logp[j + 1][i] > HMM_logp[j][iprime] - alpha * log(temp) - log(HMM_t[(f_j, e_i)]):
          HMM_logp[j + 1][i] = HMM_logp[j][iprime] - alpha * log(temp) - log(HMM_t[(f_j, e_i)])
          HMM_last_index[j + 1][i] = iprime
  curr_index = 0
  MIN_logp = 1e30
  for i in range(len(e)):
    if MIN_logp > HMM_logp[len(f)][i]:
      MIN_logp = HMM_logp[len(f)][i]
      curr_index = i
  for j in range(1, len(f) + 1)[::-1]:
    prev_index = HMM_last_index[j][curr_index]
    # print j, prev_index
    # print len(f) - j,prev_index
    sys.stdout.write("%i-%i " % (j - 1, curr_index))
    # print len(f), j-1
    # print len(e), curr_index
    # print f[j - 1], e[curr_index]
    if j != 1:
      HMM_ac_new[curr_index - prev_index] += 1
    HMM_count_fe[(f[j - 1], e[curr_index])] += 1
    curr_index = prev_index
  sys.stdout.write("\n")

  # sys.stdout.write("%i-%i " % (j, MAX_index))
  # sys.stdout.write("\n")

#Output

# dice = defaultdict(int)
# for (k, (f_i, e_j)) in enumerate(fe_count.keys()):
#   dice[(f_i,e_j)] = 2.0 * fe_count[(f_i, e_j)] / (f_count[f_i] + e_count[e_j])
#   if k % 5000 == 0:
#     sys.stderr.write(".")
# sys.stderr.write("\n")

# for (f, e) in bitext:
#   for (i, f_i) in enumerate(f): 
#     for (j, e_j) in enumerate(e):
#       if dice[(f_i,e_j)] >= opts.threshold:
#         sys.stdout.write("%i-%i " % (i,j))
#   sys.stdout.write("\n")